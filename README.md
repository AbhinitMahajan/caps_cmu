
# Emission Source Profiling using Autoencoder-based Unsupervised Learning

![Project Banner](assets/img_method.png) 

## Project Title
**Interpretable Source Separation of ACSM Spectral Data using Deep Autoencoders**

## Overview
This project aims to separate and identify emission sources from Aerosol Chemical Speciation Monitor (ACSM) spectral data collected at the Lawrenceville site in Pittsburgh. It leverages deep convolutional autoencoders to generate interpretable factor profiles, similar to traditional Positive Matrix Factorization (PMF) methods.

Developed in affiliation with the **Center for Atmospheric Particle Studies (CAPS)** at **Carnegie Mellon University**, this tool is tailored for environmental researchers interested in atmospheric source apportionment.

---

## Problem & Motivation
Traditional source separation techniques like Nonâ€‘negative Matrix Factorization (NMF) and PMF are widely used for ACSM data but are limited in capturing complex nonlinear dependencies in the spectra. This project offers a deep learningâ€“based alternative that:

- Reconstructs spectral data with high fidelity.  
- Produces **interpretable and nonâ€‘negative linear factor outputs**.  
- Incorporates tailored loss functions to maintain structural consistency and correlation patterns.

---

## âš™ï¸ Methodology
The autoencoder architecture includes:

1. **Encoder:**  
   5 convolutional blocks compress the spectral input into a lowâ€‘dimensional latent representation.

2. **Dualâ€‘Branch Decoder:**  
   - **Deep Branch:** Reconstructs the spectrum using upsampling and skip connections from the encoder.  
   - **Linear Branch:** Produces PMF-style probabilistic factor profiles via softmax-normalized global factors.

### Loss Functions
- **MSE Loss:** Supervises the deep branch for accurate spectrum reconstruction.  
- **PMF KL Loss:** Enforces probabilistic reconstruction where each sample is a mixture of global factor profiles.  
- **Consistency Loss:** Aligns deep and linear reconstructions via cosine similarity.  
- **Correlation Loss:** Preserves feature-wise correlation structure of the input spectra.  
- **Orthogonality Penalty:** Encourages diverse, non-overlapping factor profiles.  
- **Entropy Sparsity:** Promotes sharp per-sample factor assignments for clearer source attribution.

---

## Usage

### StepÂ 1: Install Dependencies
```bash
pip install -r requirements.txt
```
PythonÂ 3.8+ recommended.

### Step 2: Train the Model
```bash
python src/training.py
```
You will be prompted for:
- Raw data file containing the ACSM data (in `data/raw/`)
- Number of epochs, batch size, number of clusters or factors
- Learning rate, consistency loss weight (`lambda1`), correlation loss weight (`lambda2`)
- Linear branch regularization (`l1`, `l2`)
- **Temperature:** Controls factor profile sharpness (lower = more focused, higher = smoother). Default: 1.0
- **Orthogonality Weight:** Promotes diverse factor profiles (higher = more distinct factors). Default: 0.01
- **Entropy Weight:** Encourages sparse factor usage per sample (higher = sharper assignments). Default: 0.001

Trained artifacts are saved to:
```
saved_models/
â”œâ”€â”€ autoencoder_model.h5
â”œâ”€â”€ factor_logits_weights.npy
â””â”€â”€ probabilistic_factors.npy  # PMF-comparable factor profiles
```

### Step 3: Visualize Results
Open `test.ipynb` to:
- Load `probabilistic_factors.npy` (PMF-style factor profiles)
- Plot heatmaps and bar charts of factor profiles
- Compare with NMF profiles using correlation metrics, side-by-side visualizations, and top species analysis

---

## Project Structure
```
Unsupervised_Learning/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Raw ACSM CSV files
â”‚   â””â”€â”€ processed/          # Normalized data outputs
â”œâ”€â”€ saved_models/           # autoencoder_model.h5, linear_weights.npy
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # Seed settings & paths
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ models.py           # Encoder, decoder, autoencoder classes
â”‚   â”œâ”€â”€ training.py         # CLI for interactive training
â”‚   â””â”€â”€ visualisation.py    # Plotting utilities
â”œâ”€â”€ test.ipynb              # Notebook for evaluation & comparison
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## Contact
For questions or collaborations, reach out to Abhinit Mahajan at  
âœ‰ï¸ abhinitmahajan@cmu.edu

---

## ğŸ“„ License
This project is licensed under the MIT License. See LICENSE for details.

---

## Acknowledgements
Developed in affiliation and supervision with   
**Prof. Albert Prestro**
**Center for Atmospheric Particle Studies (CAPS)**  
**Carnegie Mellon University**
