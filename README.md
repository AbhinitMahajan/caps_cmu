
# Emission Source Profiling using Autoencoder-based Unsupervised Learning

![Project Banner](assets/img_method.png) 

## Project Title
**Interpretable Source Separation of ACSM Spectral Data using Deep Autoencoders**

## Overview
This project aims to separate and identify emission sources from Aerosol Chemical Speciation Monitor (ACSM) spectral data collected at the Lawrenceville site in Pittsburgh. It leverages deep convolutional autoencoders to generate interpretable factor profiles, similar to traditional Positive Matrix Factorization (PMF) methods.

Developed in affiliation with the **Center for Atmospheric Particle Studies (CAPS)** at **Carnegie Mellon University**, this tool is tailored for environmental researchers interested in atmospheric source apportionment.

---

## Problem & Motivation
Traditional source separation techniques like Nonâ€‘negative Matrix Factorization (NMF) and PMF are widely used for ACSM data but are limited in capturing complex nonlinear dependencies in the spectra. This project offers a deep learningâ€“based alternative that:

- Reconstructs spectral data with high fidelity.  
- Produces **interpretable and nonâ€‘negative linear factor outputs**.  
- Incorporates tailored loss functions to maintain structural consistency and correlation patterns.

---

## âš™ï¸ Methodology
The autoencoder architecture includes:

1. **Encoder:**  
   5 convolutional blocks compress the spectral input into a lowâ€‘dimensional latent representation.

2. **Dualâ€‘Branch Decoder:**  
   - **Deep Branch:** Reconstructs the spectrum using upsampling and skip connections from the encoder.  
   - **Linear Branch:** Produces a nonâ€‘negative interpretable profile mimicking PMF outputs via a constrained Dense layer.

### Loss Functions
- **MSE Loss:** Supervises the deep branch for accurate reconstruction.  
- **Consistency Loss:** Aligns the linear and nonlinear decoder outputs via a cosine similarity penalty.  
- **Correlation Loss:** Ensures the reconstructed spectra preserve the original featureâ€‘wise correlations.

---

## Usage

### StepÂ 1: Install Dependencies
```bash
pip install -r requirements.txt
```
PythonÂ 3.8+ recommended.

### StepÂ 2: Train the Model
```bash
python src/training.py
```
You will be prompted for:
- Raw data file containing the ACSM data (in `data/raw/`)
- Number of epochs, batch size, number of clusters or factors
- Learning rate, consistency loss weight (`lambda1`), correlation loss weight (`lambda2`)
- Linear branch regularization (`l1`, `l2`)

Trained artifacts are saved to:
```
saved_models/
â”œâ”€â”€ autoencoder_model.h5
â””â”€â”€ linear_weights.npy
```

### StepÂ 3: Visualize Results
Open `test.ipynb` to:
- Load `linear_weights.npy`
- Plot heatmaps and bar charts of factor profiles
- Compare with NMF profiles using correlation matrices, scatter grids, and difference/ratio charts

---

## Project Structure
```
Unsupervised_Learning/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Raw ACSM CSV files
â”‚   â””â”€â”€ processed/          # Normalized data outputs
â”œâ”€â”€ saved_models/           # autoencoder_model.h5, linear_weights.npy
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # Seed settings & paths
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ models.py           # Encoder, decoder, autoencoder classes
â”‚   â”œâ”€â”€ training.py         # CLI for interactive training
â”‚   â””â”€â”€ visualisation.py    # Plotting utilities
â”œâ”€â”€ test.ipynb              # Notebook for evaluation & comparison
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## Contact
For questions or collaborations, reach out to Abhinit Mahajan at  
âœ‰ï¸ abhinitmahajan@cmu.edu

---

## ğŸ“„ License
This project is licensed under the MIT License. See LICENSE for details.

---

## Acknowledgements
Developed in affiliation and supervision with   
**Prof. Albert Prestro**
**Center for Atmospheric Particle Studies (CAPS)**  
**Carnegie Mellon University**
